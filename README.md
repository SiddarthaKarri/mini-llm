# ğŸ§  Mini-LLM (C++ Mock Inference Engine)

**Mini-LLM** is a **minimal C++ project** that demonstrates the building blocks of a Large Language Model (LLM) inference engine.
Itâ€™s designed to be lightweight, educational, and built in under **1 hour**.

This project **does not run a real LLM**, but it mimics the pipeline:
ğŸ‘‰ **Tokenizer â†’ Embedding â†’ Matrix Multiply â†’ Softmax â†’ Token Sampling**

---

## âœ¨ Features

* âœ… **Simple whitespace tokenizer**
* âœ… **Toy embedding layer** (convert token â†’ vector)
* âœ… **Matrix multiplication + Softmax** (like transformer forward pass)
* âœ… **Random sampling from a fake vocabulary**
* âœ… **Clean C++11 implementation**, no external dependencies
* âœ… Ready for **future extensions** (quantization, PyBind11, CUDA, etc.)

---

## ğŸ“‚ Project Structure

```
mini-llm/
 â”œâ”€â”€ src/
 â”‚    â””â”€â”€ main.cpp       # Core logic
 â”œâ”€â”€ README.md           # Documentation
 â””â”€â”€ Makefile (optional) # Build automation
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repo

```bash
git clone https://github.com/SiddarthaKarri/mini-llm.git
cd mini-llm
```

### 2ï¸âƒ£ Compile

```bash
g++ src/main.cpp -o mini-llm -std=c++11
```

### 3ï¸âƒ£ Run

```bash
./mini-llm "Hello world from Sid"
```

---

## ğŸ–¥ï¸ Example Output

```
Tokens:
0: Hello
1: world
2: from
3: Karri

Generated output: [MockResponse] Sid -> model
```

---

## ğŸ” How It Works

1. **Tokenizer**
   Splits input text into tokens based on whitespace.

2. **Toy Embedding**
   Encodes the last token into a small vector (fake embedding).

3. **Linear Layer (MatMul)**
   Multiplies embedding with a random weight matrix to produce logits.

4. **Softmax**
   Converts logits into probability distribution over fake vocabulary.

5. **Sampling**
   Randomly selects the next token from vocabulary based on probabilities.

---

## ğŸ› ï¸ Future Improvements

* ğŸ”¹ Replace whitespace tokenizer with a **BPE tokenizer**
* ğŸ”¹ Implement **real transformer layers** (attention, feedforward)
* ğŸ”¹ Add **quantization** for efficiency
* ğŸ”¹ Bind with **Python (PyBind11)**
* ğŸ”¹ GPU acceleration via **CUDA**

---

## ğŸ“š Inspiration

* [llama.cpp](https://github.com/ggerganov/llama.cpp) â€“ lightweight LLM inference in pure C++
* [YALM](https://github.com/andrewkchan/yalm) â€“ Yet Another Language Model, educational C++/CUDA LLM

---

## ğŸ“œ License

MIT License â€“ free to use, modify, and learn from.

---